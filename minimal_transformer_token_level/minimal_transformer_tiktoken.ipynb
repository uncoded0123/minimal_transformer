{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu, dtype: torch.float32\n",
      "Running locally\n",
      "Loading cached tokens...\n",
      "Vocab size: 100256, dim: 512, heads: 8, layers: 6\n",
      "Total tokens: 120196189\n",
      "Using torch.compile\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# detect environment\n",
    "on_colab = 'COLAB_RELEASE_TAG' in os.environ\n",
    "on_lambda = os.path.exists('/home/ubuntu') and not on_colab\n",
    "\n",
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    data_dir = '/content/drive/MyDrive'\n",
    "    print(\"Running on Colab\")\n",
    "elif on_lambda:\n",
    "    data_dir = '/home/ubuntu'\n",
    "    print(\"Running on Lambda\")\n",
    "else:\n",
    "    data_dir = '.'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# config\n",
    "lr = 3e-4\n",
    "train_iter = 1000000\n",
    "batch_size = 32\n",
    "dim = 512\n",
    "n_heads = 8\n",
    "head_dim = dim // n_heads  # 64\n",
    "n_layers = 6\n",
    "mlp_dim = 1024\n",
    "ctx_len = 128\n",
    "plot_every = 5000\n",
    "\n",
    "# load data — WikiText-103 (Wikipedia articles, ~100M tokens)\n",
    "# cached as a tensor so it only downloads and tokenizes once\n",
    "cache_path = os.path.join(data_dir, 'wikitext103_tokens.pt')\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(\"Loading cached tokens...\")\n",
    "    tokens = torch.load(cache_path, map_location=device).long()\n",
    "else:\n",
    "    print(\"Downloading WikiText-103 and tokenizing (one-time)...\")\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    # download parquet files from HuggingFace\n",
    "    base_url = \"https://huggingface.co/api/datasets/Salesforce/wikitext/parquet/wikitext-103-raw-v1/train\"\n",
    "    parquet_files = []\n",
    "    for i in range(2):\n",
    "        fname = os.path.join(data_dir, f\"wikitext_train_{i}.parquet\")\n",
    "        if not os.path.exists(fname):\n",
    "            url = f\"{base_url}/{i}.parquet\"\n",
    "            print(f\"  downloading {url}...\")\n",
    "            subprocess.run([\"wget\", \"-q\", \"-O\", fname, url], check=True)\n",
    "        parquet_files.append(fname)\n",
    "\n",
    "    # tokenize in chunks to avoid giant Python list (~2.8GB saved)\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    chunk_size = 50000\n",
    "    chunks = []\n",
    "    for pf in parquet_files:\n",
    "        table = pq.read_table(pf)\n",
    "        texts = table.column(\"text\").to_pylist()\n",
    "        batch = []\n",
    "        for j, line in enumerate(texts):\n",
    "            if line.strip():\n",
    "                batch.extend(enc.encode(line))\n",
    "            if len(batch) >= chunk_size:\n",
    "                chunks.append(torch.tensor(batch, dtype=torch.int32))\n",
    "                batch = []\n",
    "            if j % 100000 == 0:\n",
    "                print(f\"  tokenized {j}/{len(texts)} rows from {os.path.basename(pf)}...\")\n",
    "        if batch:\n",
    "            chunks.append(torch.tensor(batch, dtype=torch.int32))\n",
    "        del texts, table\n",
    "\n",
    "    tokens = torch.cat(chunks).to(device=device).long()\n",
    "    del chunks\n",
    "    torch.save(tokens.cpu().to(torch.int32), cache_path)\n",
    "    print(f\"Saved {len(tokens)} tokens to {cache_path}\")\n",
    "\n",
    "    # clean up parquet files\n",
    "    for pf in parquet_files:\n",
    "        os.remove(pf)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encode = lambda s: enc.encode(s)\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "vocab_size = tokens.max().item() + 1\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, heads: {n_heads}, layers: {n_layers}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02\n",
    "\n",
    "init_scale = 0.1 / (2 ** 0.5)\n",
    "wq = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "wk = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "wv = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "wo = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "w1 = [torch.randn(dim, mlp_dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "w2 = [torch.randn(mlp_dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)]\n",
    "\n",
    "params = [emb, pos] + wq + wk + wv + wo + w1 + w2\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape\n",
    "    x = emb[x] + pos[:T]\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        nx = rmsnorm(x)\n",
    "        q = (nx @ wq[layer]).view(B, T, n_heads, head_dim).transpose(1, 2)\n",
    "        k = (nx @ wk[layer]).view(B, T, n_heads, head_dim).transpose(1, 2)\n",
    "        v = (nx @ wv[layer]).view(B, T, n_heads, head_dim).transpose(1, 2)\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=True).transpose(1, 2).reshape(B, T, dim)\n",
    "        x = x + attn_out @ wo[layer]\n",
    "\n",
    "        nx = rmsnorm(x)\n",
    "        out = (nx @ w1[layer]).relu() @ w2[layer]\n",
    "        x = x + out\n",
    "        \n",
    "    x = rmsnorm(x)\n",
    "    x = x @ emb.T\n",
    "    return x\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward)\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "n_tokens = len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from ./weights_tiktoken.pt\n"
     ]
    }
   ],
   "source": [
    "# load saved weights to resume training (skip this cell to train from scratch)\n",
    "weights_path = os.path.join(data_dir, 'weights_tiktoken.pt')\n",
    "ckpt = torch.load(weights_path, map_location=device)\n",
    "emb.data = ckpt['emb'].to(dtype=dtype)\n",
    "pos.data = ckpt['pos'].to(dtype=dtype)\n",
    "for i in range(n_layers):\n",
    "    wq[i].data = ckpt['wq'][i].to(dtype=dtype)\n",
    "    wk[i].data = ckpt['wk'][i].to(dtype=dtype)\n",
    "    wv[i].data = ckpt['wv'][i].to(dtype=dtype)\n",
    "    # wo[i].data = ckpt['wo'][i].to(dtype=dtype)\n",
    "    w1[i].data = ckpt['w1'][i].to(dtype=dtype)\n",
    "    w2[i].data = ckpt['w2'][i].to(dtype=dtype)\n",
    "print(f\"Loaded weights from {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "opt = torch.optim.Adam(params, lr=lr, fused=True)\n",
    "offsets = torch.arange(ctx_len, device=device)\n",
    "\n",
    "for i in range(train_iter):\n",
    "    idx = torch.randint(0, n_tokens - ctx_len, (batch_size,), device=device)\n",
    "    seqs = tokens[idx.unsqueeze(1) + offsets]\n",
    "    x = seqs[:, :-1]\n",
    "    y = seqs[:, 1:]\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "    opt.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3646, 3062]\n",
      "most important 추가Jo.ST Reading.series і grandson'M Bearingilling\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "ctx = \"most important\"\n",
    "tokens = encode(ctx)\n",
    "print(tokens)\n",
    "\n",
    "for _ in range(10):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights (persists across restarts)\n",
    "weights_path = os.path.join(data_dir, 'weights_tiktoken2.pt')\n",
    "torch.save({\n",
    "    'emb': emb.data, 'pos': pos.data,\n",
    "    'wq': [w.data for w in wq], 'wk': [w.data for w in wk],\n",
    "    'wv': [w.data for w in wv], 'wo': [w.data for w in wo],\n",
    "    'w1': [w.data for w in w1], 'w2': [w.data for w in w2],\n",
    "}, weights_path)\n",
    "\n",
    "print(f\"Saved weights to {weights_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
