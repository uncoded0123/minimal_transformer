{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b403d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=12.21\n",
      "1: loss=12.15\n",
      "2: loss=12.09\n",
      "3: loss=12.02\n",
      "4: loss=11.93\n",
      "5: loss=11.78\n",
      "6: loss=11.58\n",
      "7: loss=11.38\n",
      "8: loss=11.12\n",
      "9: loss=10.81\n",
      "10: loss=10.48\n",
      "11: loss=9.98\n",
      "12: loss=9.62\n",
      "13: loss=9.04\n",
      "14: loss=8.47\n",
      "15: loss=7.99\n",
      "16: loss=7.82\n",
      "17: loss=7.71\n",
      "18: loss=7.59\n",
      "19: loss=7.81\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# tokenizer\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "# all weights (scaled initialization)\n",
    "emb = torch.randn(200019, 64) * 0.02\n",
    "pos = torch.randn(32, 64) * 0.02\n",
    "wq = torch.randn(64, 64) * 0.1\n",
    "wk = torch.randn(64, 64) * 0.1\n",
    "wv = torch.randn(64, 64) * 0.1\n",
    "w1 = torch.randn(64, 256) * 0.1\n",
    "w2 = torch.randn(256, 64) * 0.1\n",
    "wo = torch.randn(64, 200019) * 0.02\n",
    "\n",
    "params = [emb, pos, wq, wk, wv, w1, w2, wo]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape\n",
    "    x = emb[x] + pos[:T]\n",
    "    \n",
    "    # attention (with pre-norm)\n",
    "    nx = rmsnorm(x)\n",
    "    q, k, v = nx @ wq, nx @ wk, nx @ wv\n",
    "    w = q @ k.transpose(-1, -2) / 8\n",
    "    mask = torch.triu(torch.ones(T, T), 1).bool()\n",
    "    w = w.masked_fill(mask, -1e9)\n",
    "    w = F.softmax(w, dim=-1)\n",
    "    x = x + w @ v\n",
    "    \n",
    "    # mlp (with pre-norm)\n",
    "    nx = rmsnorm(x)\n",
    "    x = x + (nx @ w1).relu() @ w2\n",
    "    \n",
    "    # output\n",
    "    return x @ wo\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tokens = torch.tensor(enc.encode(text))\n",
    "\n",
    "# train with Adam\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "opt = torch.optim.Adam(params, lr=lr)\n",
    "prev_loss = float('inf')\n",
    "\n",
    "for i in range(20):\n",
    "    starts = torch.randint(0, len(tokens) - 32, (batch_size,))\n",
    "    x = torch.stack([tokens[s:s+31] for s in starts])\n",
    "    y = torch.stack([tokens[s+1:s+32] for s in starts])\n",
    "    \n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, 200019), y.view(-1))\n",
    "    \n",
    "    if loss.item() > prev_loss:\n",
    "        lr *= 0.5\n",
    "        opt.param_groups[0]['lr'] = lr\n",
    "    prev_loss = loss.item()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f61596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen a thatly of to\n",
      "ish's, this to, my you to theI wife\n",
      " me.\n",
      "\n",
      " toUS out,,\n",
      " a me:\n",
      ",\n",
      ",\n",
      " us the that ourBut need?\n",
      "\n",
      ":\n",
      ":\n",
      " a it:\n",
      ".\n",
      " mayA as,:\n",
      "\n",
      " it take thenVUCE\n",
      " their a it thou I to you:\n",
      " not,:\n",
      " the to against it you:\n",
      ":\n",
      "?\n",
      " so thated for as,\n",
      ":\n",
      ":\n",
      " the for:\n",
      " and and a, in he a:\n",
      "With.\n",
      ".\n",
      "\n",
      " ' and.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "tokens = enc.encode(\"First Citizen\")\n",
    "\n",
    "for _ in range(100):\n",
    "    x = torch.tensor([tokens[-31:]])\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)  # temperature=0.8\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    tokens.append(next_token.item())\n",
    "\n",
    "print(enc.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
