{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu, dtype: torch.float32\n",
      "Vocab size: 65, dim: 128, layers: 2\n",
      "Using torch.compile\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "0: loss=4.25\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n",
      "x.shape=torch.Size([64, 31, 128]) emb.T.shape=torch.Size([128, 65])\n",
      "logits.shape=torch.Size([64, 31, 65])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 213\u001b[0m\n\u001b[1;32m    210\u001b[0m x \u001b[38;5;241m=\u001b[39m all_seqs[idx, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# shape (batch_size, ctx_len-1) grabs random row, all elements in that row except the last, to predict all row elements in y (y is same indeices in x but shifted by + 1)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m y \u001b[38;5;241m=\u001b[39m all_seqs[idx, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;66;03m# grabs same row as x, grabs same elements as x but shifted by + 1, so each element becomes what the transformer should predict, aka next token prediction. each y element is the correct next token.\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m logits \u001b[38;5;241m=\u001b[39m forward(x) \u001b[38;5;66;03m# raw results from the transformer. shape (B, T, vocab_size) because forward returns x @ emb.T and x is (B, T, 128) and emb.T is (128, vocab_size)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    215\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmsnorm\u001b[39m(x, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m/\u001b[39m ((x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m+\u001b[39m eps)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(x):\n\u001b[1;32m     80\u001b[0m     B, T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m emb[x] \u001b[38;5;241m+\u001b[39m pos[:T] \u001b[38;5;66;03m# C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1209\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1208\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:315\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     ), torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 315\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m    316\u001b[0m             compiled_fn, args_, disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp, steal_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     grad_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1937\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   1928\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39mfwd_rng_states)\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[0;32m-> 1937\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m   1938\u001b[0m     CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_fw,\n\u001b[1;32m   1939\u001b[0m     args,\n\u001b[1;32m   1940\u001b[0m     disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   1941\u001b[0m )\n\u001b[1;32m   1943\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   1944\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    488\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    489\u001b[0m         runtime_metadata,\n\u001b[1;32m    490\u001b[0m         out,\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(runtime_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:689\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    686\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    687\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 689\u001b[0m outs \u001b[38;5;241m=\u001b[39m compiled_fn(args)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:460\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable(inputs)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/.cache/pip_tmp/torchinductor_cody/5a/c5anv4esj5r4isvnkb7cdjmoystl7uj4pqbbuejc3h6c3vujboee.py:762\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    760\u001b[0m buf3 \u001b[38;5;241m=\u001b[39m empty_strided_cpu((\u001b[38;5;241m1984\u001b[39m, \u001b[38;5;241m128\u001b[39m), (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [q], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m extern_kernels\u001b[38;5;241m.\u001b[39mmm(reinterpret_tensor(buf2, (\u001b[38;5;241m1984\u001b[39m, \u001b[38;5;241m128\u001b[39m), (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m), primals_4, out\u001b[38;5;241m=\u001b[39mbuf3)\n\u001b[1;32m    763\u001b[0m buf4 \u001b[38;5;241m=\u001b[39m empty_strided_cpu((\u001b[38;5;241m1984\u001b[39m, \u001b[38;5;241m128\u001b[39m), (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [k], Original ATen: [aten.mm]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "batch_size = 64\n",
    "dim = 128 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights\n",
    "\n",
    "\n",
    "# 1st Block Weights\n",
    "# -----------------------------------------------\n",
    "# 1st attention\n",
    "wq0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for q, input dim is rows and it's for dot producting with nx (RMS normalized x) row vector\n",
    "wk0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for k, same as q\n",
    "wv0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for v, same as q and k\n",
    "\n",
    "# 1st MLP\n",
    "w10 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP input to hidden layer\n",
    "w20 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2nd Block Weights\n",
    "# -----------------------------------------------\n",
    "# 2nd Attention\n",
    "wq1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # takes in the output from 1st block\n",
    "wk1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1\n",
    "wv1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1 and wk1\n",
    "\n",
    "# 2nd MLP\n",
    "w11 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, input to hidden layer\n",
    "w21 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "\n",
    "\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1st Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    # Formula for attention scores: (Q @ K^T) / sqrt(d_k), where Q is the query matrix, K is the key matrix, and d_k is the dimension of the key vectors. This formula computes the attention scores by taking the dot product of the query and key matrices, and then scaling it by the square root of the dimension of the key vectors to prevent large values that can lead to exploding softmax\n",
    "    q, k, v = nx @ wq0, nx @ wk0, nx @ wv0\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5) # actual attention calculations\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # creates matrix filled with 1s in the lower triangle and diagonal, and 0s in the upper triangle.\n",
    "    scores = scores.masked_fill(~mask, float('-inf')) # ~ is bitwise not. Here it flips the mask so that the lower triangle and diagonal are False, and the upper triangle is True. Then masked_fill replaces the True values (upper triangle) with -inf, which effectively masks out those positions in the attention scores. Then it leaves everything else alone\n",
    "    sm = F.softmax(scores, dim=-1) # probabilities per row\n",
    "    attn_out = sm @ v # shape (B, T, C) because sm is (B, T, T) and v is (B, T, C). This is the output of the attention mechanism\n",
    "    x = x + attn_out # residual connection, adds attn input to attn output, shape (B, T, 128)\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w10 # shape (B, T, 256)\n",
    "    out = out.relu() # shape (B, T, 256), applies ReLU activation function element-wise\n",
    "    out = out @ w20 # shape (B, T, 128), this is the output of the MLP\n",
    "    x = x + out # residual connection, adds mlp input to mlp output, shape (B, T, 128)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2nd Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x) # x is input from 1st block, shape (B, T, 128)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq1, nx @ wk1, nx @ wv1\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    sm = F.softmax(scores, dim=-1)\n",
    "    attn_out = sm @ v\n",
    "    x = x + attn_out\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w11\n",
    "    out = out.relu()\n",
    "    out = out @ w21\n",
    "    x = x + out\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    print(f'{x.shape=}', f'{emb.T.shape=}') # x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    x = x @ emb.T # x is prediction vector, that vector dots with emb.T to get similarity scores (dot product), which is most similar can be next token prediction. shape (B, T, vocab_size) because x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    return x\n",
    "    \n",
    "    '''\n",
    "    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\n",
    "    Say dim=3, vocab=3 for simplicity:\n",
    "\n",
    "    emb = [[0.2, 0.5, 0.1],   # token \"a\"\n",
    "        [0.9, 0.1, 0.3],   # token \"b\"  \n",
    "        [0.1, 0.8, 0.2]]   # token \"c\"\n",
    "    Output vector after all blocks: x = [0.85, 0.15, 0.28]\n",
    "\n",
    "    x @ emb.T = dot product of x with each row:\n",
    "\n",
    "    vs \"a\": 0.85*0.2 + 0.15*0.5 + 0.28*0.1 = 0.27\n",
    "    vs \"b\": 0.85*0.9 + 0.15*0.1 + 0.28*0.3 = 0.86 ← highest\n",
    "    vs \"c\": 0.85*0.1 + 0.15*0.8 + 0.28*0.2 = 0.26\n",
    "    Result: [0.27, 0.86, 0.26] → model predicts \"b\" because x is most similar to \"b\"'s embedding.\n",
    "\n",
    "    The blocks learned to push x toward the embedding of the correct next token.\n",
    "    '''\n",
    "\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward) # compiles forward, removes python interpretor, looks at all the ops, makes less trips between vram and gpu cores with kernel fusion where instead of doing a calculation for 1 var by carrying data from vram to gpu cores doing an intermediate calculation then bringing  result back to vram then taking that answer back to cores for next operation then returning to vram over and over it fuses (combines) all those operations into 1 transfer then does the calculations then returns result to vram\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device) # encoding text to list of numbers (token ids), converting that list to a tensor\n",
    "'''\n",
    "all_seqs = [] # stores tensors in this list. shape eg. (1000000, 32) aka (all tokens in file, context len) stores like lists of all context size sequences in the fiile. So if ctx_len is 32, it stores all sequences of 32 tokens (sliding 1 token at a time) in the file, which are the training examples. Each sequence is a sequence of token ids (integers).\n",
    "for i in range(len(tokens) - ctx_len + 1): # same pattern as convolutional kernal... len of total elements minus window (or convolutional kernel) size + 1.\n",
    "    all_seqs.append(tokens[i : i + ctx_len]) # ctx len is how wide is that window or kernel. i changes by step size. makes list of tokens 0 to 31, appends all those to all_seq list, next iter looks at tokens 1 to 32, appends that list as next row of tokens, etc.\n",
    "all_seqs = torch.stack(all_seqs) # converts list of seperate tensors to one matrix tensor\n",
    "'''\n",
    "all_seqs = tokens.unfold(dimension=0, size=ctx_len, step=1) # same as torch.stack, just faster. shape (kind of like cnn number of chars in whole file like 1115390 minus window size like 32 and + 1 for the starting window position, ctx_len)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True) # creates object instance of Adam. optimizer = update like w -= lr * grad, except adam does other details too. fused kernal aka looks at all calculations and carries data for processing all before returning results to vram instead of doing one op then returning result then doing next op combined with previous result carried back from vram so basically less trips.\n",
    "'''\n",
    "Adam = Adaptive + Momentum combined. Momentum is like a running average of the gradients, so it helps to smooth out the updates and can help to accelerate convergence in the relevant direction and dampen oscillations. The momentum term is calculated as m = b_1 * m + (1 - b_1) * grad, where m is the momentum, b_1 is the momentum decay factor (usually around 0.9), and grad is the current gradient. The adaptive part (Ada) adjusts the learning rate for each parameter based on the historical gradients, which helps to improve convergence.\n",
    "eg. of momentum with how much of old momentum to keep = b_1 = beta sub 1 = 0.9...\n",
    "# Adam formula, for updating parameter:\n",
    "param -= lr * m / (√v + ε)\n",
    "\n",
    "\n",
    "m = b_1 * m + (1 - b_1) * grad\n",
    "v = b_2 * v + (1 - b_2) * (grad ** 2)\n",
    "grad sequence: [4, 2, 6, 1]\n",
    "\n",
    "Step 1: m = 0.9(0)   + 0.1(4) = 0.4\n",
    "Step 2: m = 0.9(0.4) + 0.1(2) = 0.56\n",
    "Step 3: m = 0.9(0.56)+ 0.1(6) = 1.104\n",
    "Step 4: m = 0.9(1.104)+0.1(1) = 1.094\n",
    "\n",
    "\n",
    "\n",
    "m = momentum (direction to go)\n",
    "√v = adapts step size (Ada part)\n",
    "ε = tiny number to avoid division by zero (~1e-8)\n",
    "lr = learning rate\n",
    "Adam = Adaptive + Momentum combined.\n",
    "'''\n",
    "\n",
    "\n",
    "for i in range(10000): # number of training iterations, each iteration is one batch of training examples\n",
    "    idx = torch.randint(0, all_seqs.shape[0], (batch_size,)) # random rows of context to process. also, part of the benifit of looking at parts of instead of the whole file is kind of like cnn translation invariance where it can learn patterns regardless of where they are in the context window, like learning cat ear shape no matter it is in an image. shape (batch_size,) because it's a list of row indices to select for the batch\n",
    "    x = all_seqs[idx, :-1] # shape (batch_size, ctx_len-1) grabs random row, all elements in that row except the last, to predict all row elements in y (y is same indeices in x but shifted by + 1)\n",
    "    y = all_seqs[idx, 1:] # grabs same row as x, grabs same elements as x but shifted by + 1, so each element becomes what the transformer should predict, aka next token prediction. each y element is the correct next token.\n",
    "\n",
    "    logits = forward(x) # raw results from the transformer. makes tensors same shape, ignores dim 0, does regular matmul, then puts dim 0 in resulting tensor. shape (batch_size, ctx_len-1, vocab_size) because forward returns x @ emb.T and x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    print(f'{logits.shape=}')\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1)) # logits becomes shape rows = every prediction in batch and every context len prediction in that batch, columns are each vocab letter, y indexes into the target index of each prediction. then each of those goes into negative natural log, then all the outputs are averaged, that is the loss.\n",
    "\n",
    "    opt.zero_grad() # makes all grads 0 so they don't add (or accumulate) to previous batch of grads\n",
    "    loss.backward()\n",
    "    opt.step() #updates weights with grads from loss.backward()\n",
    "\n",
    "    if i % 100 == 0: # every 100 iterations print loss\n",
    "        print(f\"{i}: loss={loss.item():.2f}\") # loss.item() gets the scalar value of the loss tensor for printing, and formats it to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst Citizen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m encode(ctx)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode' is not defined"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "print(tokens)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115359, 32]), torch.Size([1115390]), 1115390)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seqs.shape, tokens.shape, len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 31])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
