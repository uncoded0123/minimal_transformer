{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "batch_size = 64\n",
    "dim = 128 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 token in the embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights\n",
    "\n",
    "\n",
    "# 1st Block Weights\n",
    "# -----------------------------------------------\n",
    "# 1st attention\n",
    "wq0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for q, input dim is rows and it's for dot producting with nx (RMS normalized x) row vector\n",
    "wk0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for k, same as q\n",
    "wv0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for v, same as q and v\n",
    "\n",
    "# 1st MLP\n",
    "w10 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP input to hidden layer\n",
    "w20 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2nd Block Weights\n",
    "# -----------------------------------------------\n",
    "# 2nd Attention\n",
    "wq1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # takes in the output from block 0\n",
    "wk1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1\n",
    "wv1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1 and wk1\n",
    "\n",
    "# 2nd MLP\n",
    "w11 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in block 1, input to hidden layer\n",
    "w21 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in block 1, hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] â€” the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # includes up to but not including T (slicing rules)\n",
    "\n",
    "\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1st Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq0, nx @ wk0, nx @ wv0\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    x = x + F.softmax(scores, dim=-1) @ v\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    x = x + (nx @ w10).relu() @ w20\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2nd Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq1, nx @ wk1, nx @ wv1\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    x = x + F.softmax(scores, dim=-1) @ v\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    x = x + (nx @ w11).relu() @ w21\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    return x @ emb.T\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward)\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical way (nn.Module + nn.Linear instead of raw weight dicts)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, dim, bias=False)\n",
    "        self.w1 = nn.Linear(dim, mlp_dim, bias=False)\n",
    "        self.w2 = nn.Linear(mlp_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        nx = rmsnorm(x)\n",
    "        q, k, v = self.wq(nx), self.wk(nx), self.wv(nx)\n",
    "        x = x + F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        x = x + self.w2(F.relu(self.w1(rmsnorm(x))))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, n_layers, mlp_dim, ctx_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos = nn.Embedding(ctx_len, dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(dim, mlp_dim) for _ in range(n_layers)])\n",
    "        self.unemb = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.emb(x) + self.pos(torch.arange(T, device=x.device))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.unemb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical way: instantiate + train\n",
    "\n",
    "model = Transformer(vocab_size, dim, n_layers, mlp_dim, ctx_len).to(device)\n",
    "opt2 = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (32,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt2.zero_grad()\n",
    "    loss.backward()\n",
    "    opt2.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=4.18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_probs[torch\u001b[38;5;241m.\u001b[39marange(y\u001b[38;5;241m.\u001b[39mnumel(), device\u001b[38;5;241m=\u001b[39mdevice), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    115\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 116\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    117\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from scratch: raw operations, no abstractions\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# config\n",
    "dim = 128\n",
    "n_layers = 2\n",
    "mlp_dim = 256\n",
    "ctx_len = 32\n",
    "\n",
    "# data + tokenizer\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # 65\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# weights (raw tensors, no dicts, no nn.Module)\n",
    "emb = torch.randn(65, 128, device=device) * 0.02\n",
    "pos = torch.randn(32, 128, device=device) * 0.02\n",
    "\n",
    "# layer 0 weights\n",
    "wq0 = torch.randn(128, 128, device=device) * 0.02\n",
    "wk0 = torch.randn(128, 128, device=device) * 0.02\n",
    "wv0 = torch.randn(128, 128, device=device) * 0.02\n",
    "w10 = torch.randn(128, 256, device=device) * 0.02\n",
    "w20 = torch.randn(256, 128, device=device) * 0.02\n",
    "\n",
    "# layer 1 weights\n",
    "wq1 = torch.randn(128, 128, device=device) * 0.02\n",
    "wk1 = torch.randn(128, 128, device=device) * 0.02\n",
    "wv1 = torch.randn(128, 128, device=device) * 0.02\n",
    "w11 = torch.randn(128, 256, device=device) * 0.02\n",
    "w21 = torch.randn(256, 128, device=device) * 0.02\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape\n",
    "\n",
    "    # embed: lookup rows from emb table + add position vectors\n",
    "    x = emb[x] + pos[:T]\n",
    "\n",
    "    # --- block 0 ---\n",
    "    # rmsnorm before attention\n",
    "    rms = (x ** 2).mean(dim=-1, keepdim=True).sqrt() + 1e-5\n",
    "    nx = x / rms\n",
    "\n",
    "    # q, k, v projections\n",
    "    q = nx @ wq0\n",
    "    k = nx @ wk0\n",
    "    v = nx @ wv0\n",
    "\n",
    "    # attention scores\n",
    "    scores = q @ k.transpose(-2, -1) / (dim ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    weights = scores.exp() / scores.exp().sum(dim=-1, keepdim=True)\n",
    "    x = x + weights @ v\n",
    "\n",
    "    # rmsnorm before MLP\n",
    "    rms = (x ** 2).mean(dim=-1, keepdim=True).sqrt() + 1e-5\n",
    "    nx = x / rms\n",
    "\n",
    "    # MLP\n",
    "    hidden = (nx @ w10).clamp(min=0)\n",
    "    x = x + hidden @ w20\n",
    "\n",
    "    # --- block 1 ---\n",
    "    rms = (x ** 2).mean(dim=-1, keepdim=True).sqrt() + 1e-5\n",
    "    nx = x / rms\n",
    "\n",
    "    q = nx @ wq1\n",
    "    k = nx @ wk1\n",
    "    v = nx @ wv1\n",
    "\n",
    "    scores = q @ k.transpose(-2, -1) / (dim ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    weights = scores.exp() / scores.exp().sum(dim=-1, keepdim=True)\n",
    "    x = x + weights @ v\n",
    "\n",
    "    rms = (x ** 2).mean(dim=-1, keepdim=True).sqrt() + 1e-5\n",
    "    nx = x / rms\n",
    "\n",
    "    hidden = (nx @ w11).clamp(min=0)\n",
    "    x = x + hidden @ w21\n",
    "\n",
    "    # unembed\n",
    "    logits = x @ emb.T\n",
    "    return logits\n",
    "\n",
    "# train\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "opt = torch.optim.Adam(params, lr=1e-4)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (32,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "\n",
    "    # cross entropy: -log(probability of correct token)\n",
    "    log_probs = logits.view(-1, vocab_size) - logits.view(-1, vocab_size).logsumexp(dim=-1, keepdim=True)\n",
    "    loss = -log_probs[torch.arange(y.numel(), device=device), y.view(-1)].mean()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")\n",
    "\n",
    "# generate\n",
    "toks = encode(\"First Citizen\")\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([toks[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    scores = logits[0, -1] / 0.8\n",
    "    probs = scores.exp() / scores.exp().sum()\n",
    "    toks.append(torch.multinomial(probs, 1).item())\n",
    "print(decode(toks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
