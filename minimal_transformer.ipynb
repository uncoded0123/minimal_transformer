{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu, dtype: torch.float32\n",
      "Vocab size: 65, dim: 128, layers: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 166\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    Say dim=3, vocab=3 for simplicity:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    The blocks learned to push x toward the embedding of the correct next token.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompile\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m     forward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(forward) \u001b[38;5;66;03m# torch compile makes less trips to and from vram, and optimizes the forward function in various ways, like fusing operations together, which can speed up training\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode(text), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/__init__.py:2572\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     backend \u001b[38;5;241m=\u001b[39m _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001b[0;32m-> 2572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m   2573\u001b[0m     backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m   2574\u001b[0m     nopython\u001b[38;5;241m=\u001b[39mfullgraph,\n\u001b[1;32m   2575\u001b[0m     dynamic\u001b[38;5;241m=\u001b[39mdynamic,\n\u001b[1;32m   2576\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m   2577\u001b[0m )(model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/__init__.py:2686\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;66;03m# Lazy modules\u001b[39;00m\n\u001b[1;32m   2685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[0;32m-> 2686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/__init__.py:53\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_break_reasons, guard_failures, orig_code_map, reset_frame_count\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Register polyfill functions\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfills\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loader \u001b[38;5;28;01mas\u001b[39;00m _  \u001b[38;5;66;03m# usort: skip # noqa: F401\u001b[39;00m\n\u001b[1;32m     56\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_in_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massume_constant_result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m ]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# allowlist this for weights_only load of NJTs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/polyfills/loader.py:25\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     26\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, package\u001b[38;5;241m=\u001b[39mpolyfills\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/polyfills/loader.py:26\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 26\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, package\u001b[38;5;241m=\u001b[39mpolyfills\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/polyfills/builtins.py:30\u001b[0m\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menumerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     27\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;129m@substitute_in_graph\u001b[39m(builtins\u001b[38;5;241m.\u001b[39mall, can_constant_fold_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(iterable: Iterable[\u001b[38;5;28mobject\u001b[39m], \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m elem:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/decorators.py:427\u001b[0m, in \u001b[0;36msubstitute_in_graph.<locals>.wrapper\u001b[0;34m(traceable_fn)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(original_fn) \u001b[38;5;129;01min\u001b[39;00m _polyfilled_function_ids:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate polyfilled object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 427\u001b[0m rule_map: \u001b[38;5;28mdict\u001b[39m[Any, \u001b[38;5;28mtype\u001b[39m[VariableTracker]] \u001b[38;5;241m=\u001b[39m get_torch_obj_rule_map()\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_fn \u001b[38;5;129;01min\u001b[39;00m rule_map:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different rules: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPolyfilledFunctionVariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule_map[original_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py:2870\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[0;32m-> 2870\u001b[0m         obj \u001b[38;5;241m=\u001b[39m load_object(k)\n\u001b[1;32m   2871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _module_dir(torch) \u001b[38;5;241m+\u001b[39m k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/\u001b[39m\u001b[38;5;124m\"\u001b[39m) :]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py:2901\u001b[0m, in \u001b[0;36mload_object\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2900\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2901\u001b[0m         val \u001b[38;5;241m=\u001b[39m _load_obj_from_str(x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2902\u001b[0m     val \u001b[38;5;241m=\u001b[39m unwrap_if_wrapper(val)\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py:2885\u001b[0m, in \u001b[0;36m_load_obj_from_str\u001b[0;34m(fully_qualified_name)\u001b[0m\n\u001b[1;32m   2883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[1;32m   2884\u001b[0m     module, obj_name \u001b[38;5;241m=\u001b[39m fully_qualified_name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(importlib\u001b[38;5;241m.\u001b[39mimport_module(module), obj_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/map.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suspend_functionalization\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, create_joint\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     _has_potential_branch_input_alias,\n\u001b[1;32m      9\u001b[0m     _has_potential_branch_input_mutation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     UnsupportedAliasMutationException,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HigherOrderOperator\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:135\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraced_function_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     aot_dispatch_subclass,\n\u001b[1;32m    114\u001b[0m     create_functional_call,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     fn_prepped_for_autograd,\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     _get_autocast_states,\n\u001b[1;32m    123\u001b[0m     _get_symint_hints,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     strict_zip,\n\u001b[1;32m    134\u001b[0m )\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_partition\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mzip\u001b[39m \u001b[38;5;241m=\u001b[39m strict_zip\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# This global counter increments every time we compile a graph with\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# AOTAutograd.  You can use this to correlate runtime error messages\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# with compile time (e.g., if you get an error at runtime saying\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# one counter is allocated per entire compiled block (but this block\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# may involve compiling multiple subgraphs; e.g., for forwards/backwards)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/partitioners.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckpointPolicy\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_info_provider\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphInfoProvider\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     dp_knapsack,\n\u001b[1;32m     40\u001b[0m     greedy_knapsack,\n\u001b[1;32m     41\u001b[0m     ilp_knapsack,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack_evaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnapsackEvaluator\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_activation_checkpointing/graph_info_provider.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, Node\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGraphInfoProvider\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/networkx/__init__.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadwrite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Need to test with SciPy, when available\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/networkx/algorithms/__init__.py:65\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwiener\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Make certain subpackages available to the user as direct imports from\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# the `networkx` namespace.\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m approximation\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assortativity\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bipartite\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/networkx/algorithms/approximation/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapproximation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraveling_salesman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapproximation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreewidth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapproximation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertex_cover\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapproximation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaxcut\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1187\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "batch_size = 64\n",
    "dim = 128 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights\n",
    "\n",
    "\n",
    "# 1st Block Weights\n",
    "# -----------------------------------------------\n",
    "# 1st attention\n",
    "wq0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for q, input dim is rows and it's for dot producting with nx (RMS normalized x) row vector\n",
    "wk0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for k, same as q\n",
    "wv0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for v, same as q and k\n",
    "\n",
    "# 1st MLP\n",
    "w10 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP input to hidden layer\n",
    "w20 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2nd Block Weights\n",
    "# -----------------------------------------------\n",
    "# 2nd Attention\n",
    "wq1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # takes in the output from 1st block\n",
    "wk1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1\n",
    "wv1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1 and wk1\n",
    "\n",
    "# 2nd MLP\n",
    "w11 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, input to hidden layer\n",
    "w21 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "\n",
    "\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1st Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    # Formula for attention scores: (Q @ K^T) / sqrt(d_k), where Q is the query matrix, K is the key matrix, and d_k is the dimension of the key vectors. This formula computes the attention scores by taking the dot product of the query and key matrices, and then scaling it by the square root of the dimension of the key vectors to prevent large values that can lead to exploding softmax\n",
    "    q, k, v = nx @ wq0, nx @ wk0, nx @ wv0\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5) # actual attention calculations\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # creates matrix filled with 1s in the lower triangle and diagonal, and 0s in the upper triangle.\n",
    "    scores = scores.masked_fill(~mask, float('-inf')) # ~ is bitwise not. Here it flips the mask so that the lower triangle and diagonal are False, and the upper triangle is True. Then masked_fill replaces the True values (upper triangle) with -inf, which effectively masks out those positions in the attention scores. Then it leaves everything else alone\n",
    "    sm = F.softmax(scores, dim=-1) # probabilities per row\n",
    "    attn_out = sm @ v # shape (B, T, C) because sm is (B, T, T) and v is (B, T, C). This is the output of the attention mechanism\n",
    "    x = x + attn_out # residual connection, adds attn input to attn output, shape (B, T, 128)\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w10 # shape (B, T, 256)\n",
    "    out = out.relu() # shape (B, T, 256), applies ReLU activation function element-wise\n",
    "    out = out @ w20 # shape (B, T, 128), this is the output of the MLP\n",
    "    x = x + out # residual connection, adds mlp input to mlp output, shape (B, T, 128)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2nd Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x) # x is input from 1st block, shape (B, T, 128)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq1, nx @ wk1, nx @ wv1\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    sm = F.softmax(scores, dim=-1)\n",
    "    attn_out = sm @ v\n",
    "    x = x + attn_out\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w11\n",
    "    out = out.relu()\n",
    "    out = out @ w21\n",
    "    x = x + out\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    x = x @ emb.T # x is prediction vector, that vector dots with emb.T to get similarity scores (dot product), which is most similar can be next token prediction. shape (B, T, vocab_size) because x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    return x\n",
    "    \n",
    "    '''\n",
    "    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\n",
    "    Say dim=3, vocab=3 for simplicity:\n",
    "\n",
    "    emb = [[0.2, 0.5, 0.1],   # token \"a\"\n",
    "        [0.9, 0.1, 0.3],   # token \"b\"  \n",
    "        [0.1, 0.8, 0.2]]   # token \"c\"\n",
    "    Output vector after all blocks: x = [0.85, 0.15, 0.28]\n",
    "\n",
    "    x @ emb.T = dot product of x with each row:\n",
    "\n",
    "    vs \"a\": 0.85*0.2 + 0.15*0.5 + 0.28*0.1 = 0.27\n",
    "    vs \"b\": 0.85*0.9 + 0.15*0.1 + 0.28*0.3 = 0.86 ← highest\n",
    "    vs \"c\": 0.85*0.1 + 0.15*0.8 + 0.28*0.2 = 0.26\n",
    "    Result: [0.27, 0.86, 0.26] → model predicts \"b\" because x is most similar to \"b\"'s embedding.\n",
    "\n",
    "    The blocks learned to push x toward the embedding of the correct next token.\n",
    "    '''\n",
    "\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward) # torch compile makes less trips to and from vram, and optimizes the forward function in various ways, like fusing operations together, which can speed up training\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52]\n",
      "First CitizenazpAdk.QEtxgXv!o.Zm.\n",
      "byJNz!rdb&-s,oFIBmggf&VKgy!yveN3aitkZv'IsCaita3&oRcmF?qdSl;M:y\n",
      "NQatBe'IBS&$ \n",
      "u?kN.kYxKxcRmmQuGua3bhomqxV&KADm;ipwZ-d-Tv-!uVfyoSDauzmySA;.MAUcCZu:ruNyRzcomLoOuNDuNNcHb-im:BATdSHEi,pUEyc\n",
      "?O!?bCbW,P'oDI PSrMXvqMfTvT$B!i!Naw!tvf.AvBKggAiTmTZzZWOpdd\n",
      " yksZdW?xxyy;u,-$eA:?eE- deuuZzDJ3bTpx ;rNBtdyomKB;!RfaQYeY'NV&maH\n",
      "gmQMeD,!nqMKQ\n",
      "t3y;E?J?Qpxj&jnsIKjv$!EJRtcxnb!ah\n",
      "H!YZ:,k.\n",
      "$EkEA&VYNqSuW ;yPew!WB-LVJFKgRQ\n",
      "IwJRbokVtamXbWhy3ciBv?WFHGvcJ;Q$CZtyIGjPaE!AvrQjkO!vg.NjGJn&meWb:LEtpcX3xXQ?l!\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "print(tokens)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
