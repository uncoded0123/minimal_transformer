{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# uncomment for file download:\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "train_iter = 1000000\n",
    "batch_size = 64\n",
    "dim = 1024 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "plot_every = 5000 # plot embedding map every N iterations\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights — lists of weight matrices, one per layer\n",
    "# each layer gets its own wq, wk, wv (attention) and w1, w2 (MLP)\n",
    "init_scale = 0.1 / (2 ** 0.5)\n",
    "wq = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wq[layer] = query weight matrix\n",
    "wk = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wk[layer] = key weight matrix\n",
    "wv = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wv[layer] = value weight matrix\n",
    "w1 = [torch.randn(dim, mlp_dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # w1[layer] = MLP input→hidden\n",
    "w2 = [torch.randn(mlp_dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # w2[layer] = MLP hidden→output\n",
    "\n",
    "\n",
    "params = [emb, pos] + wq + wk + wv + w1 + w2\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # causal mask, reused across all layers\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        # Norm\n",
    "        nx = rmsnorm(x)\n",
    "\n",
    "        # Attention\n",
    "        q, k, v = nx @ wq[layer], nx @ wk[layer], nx @ wv[layer]\n",
    "        scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attn_out = F.softmax(scores, dim=-1) @ v\n",
    "        x = x + attn_out # residual connection\n",
    "\n",
    "        # Norm\n",
    "        nx = rmsnorm(x)\n",
    "\n",
    "        # MLP\n",
    "        out = (nx @ w1[layer]).relu() @ w2[layer]\n",
    "        x = x + out # residual connection\n",
    "\n",
    "    x = x @ emb.T # similarity scores against all token embeddings → logits\n",
    "    return x\n",
    "    \n",
    "    '''\n",
    "    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\n",
    "    Say dim=3, vocab=3 for simplicity:\n",
    "\n",
    "    emb = [[0.2, 0.5, 0.1],   # token \"a\"\n",
    "        [0.9, 0.1, 0.3],   # token \"b\"  \n",
    "        [0.1, 0.8, 0.2]]   # token \"c\"\n",
    "    Output vector after all blocks: x = [0.85, 0.15, 0.28]\n",
    "\n",
    "    x @ emb.T = dot product of x with each row:\n",
    "\n",
    "    vs \"a\": 0.85*0.2 + 0.15*0.5 + 0.28*0.1 = 0.27\n",
    "    vs \"b\": 0.85*0.9 + 0.15*0.1 + 0.28*0.3 = 0.86 ← highest\n",
    "    vs \"c\": 0.85*0.1 + 0.15*0.8 + 0.28*0.2 = 0.26\n",
    "    Result: [0.27, 0.86, 0.26] → model predicts \"b\" because x is most similar to \"b\"'s embedding.\n",
    "\n",
    "    The blocks learned to push x toward the embedding of the correct next token.\n",
    "    '''\n",
    "\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward) # compiles forward, removes python interpretor, looks at all the ops, makes less trips between vram and gpu cores with kernel fusion where instead of doing a calculation for 1 var by carrying data from vram to gpu cores doing an intermediate calculation then bringing  result back to vram then taking that answer back to cores for next operation then returning to vram over and over it fuses (combines) all those operations into 1 transfer then does the calculations then returns result to vram\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device) # encoding text to list of numbers (token ids), converting that list to a tensor\n",
    "'''\n",
    "all_seqs = [] # stores tensors in this list. shape eg. (1000000, 32) aka (all tokens in file, context len) stores like lists of all context size sequences in the fiile. So if ctx_len is 32, it stores all sequences of 32 tokens (sliding 1 token at a time) in the file, which are the training examples. Each sequence is a sequence of token ids (integers).\n",
    "for i in range(len(tokens) - ctx_len + 1): # same pattern as convolutional kernal... len of total elements minus window (or convolutional kernel) size + 1.\n",
    "    all_seqs.append(tokens[i : i + ctx_len]) # ctx len is how wide is that window or kernel. i changes by step size. makes list of tokens 0 to 31, appends all those to all_seq list, next iter looks at tokens 1 to 32, appends that list as next row of tokens, etc.\n",
    "all_seqs = torch.stack(all_seqs) # converts list of seperate tensors to one matrix tensor\n",
    "'''\n",
    "all_seqs = tokens.unfold(dimension=0, size=ctx_len, step=1) # same as torch.stack, just faster. shape (kind of like cnn number of chars in whole file like 1115390 minus window size like 32 and + 1 for the starting window position, ctx_len)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True) # creates object instance of Adam. optimizer = update like w -= lr * grad, except adam does other details too. fused kernal aka looks at all calculations and carries data for processing all before returning results to vram instead of doing one op then returning result then doing next op combined with previous result carried back from vram so basically less trips.\n",
    "'''\n",
    "Adam = Adaptive + Momentum combined. Momentum is like a running average of the gradients, so it helps to smooth out the updates and can help to accelerate convergence in the relevant direction and dampen oscillations. The momentum term is calculated as m = b_1 * m + (1 - b_1) * grad, where m is the momentum, b_1 is the momentum decay factor (usually around 0.9), and grad is the current gradient. The adaptive part (Ada) adjusts the learning rate for each parameter based on the historical gradients, which helps to improve convergence.\n",
    "eg. of momentum with how much of old momentum to keep = b_1 = beta sub 1 = 0.9...\n",
    "# Adam formula, for updating parameter:\n",
    "param -= lr * m / (√v + ε)\n",
    "\n",
    "\n",
    "m = b_1 * m + (1 - b_1) * grad\n",
    "v = b_2 * v + (1 - b_2) * (grad ** 2)\n",
    "grad sequence: [4, 2, 6, 1]\n",
    "\n",
    "Step 1: m = 0.9(0)   + 0.1(4) = 0.4\n",
    "Step 2: m = 0.9(0.4) + 0.1(2) = 0.56\n",
    "Step 3: m = 0.9(0.56)+ 0.1(6) = 1.104\n",
    "Step 4: m = 0.9(1.104)+0.1(1) = 1.094\n",
    "\n",
    "\n",
    "\n",
    "m = momentum (direction to go)\n",
    "√v = adapts step size (Ada part)\n",
    "ε = tiny number to avoid division by zero (~1e-8)\n",
    "lr = learning rate\n",
    "Adam = Adaptive + Momentum combined.\n",
    "'''\n",
    "\n",
    "def plot_embeddings(step, loss_val):\n",
    "    E = emb.detach().float().cpu()\n",
    "    E = E - E.mean(dim=0)\n",
    "    U, S, V = torch.svd(E)\n",
    "    coords = E @ V[:, :2]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(coords[:, 0].numpy(), coords[:, 1].numpy(), s=20)\n",
    "    for i, c in itos.items():\n",
    "        label = repr(c) if c in (' ', '\\n', '\\t') else c\n",
    "        plt.annotate(label, (coords[i, 0].item(), coords[i, 1].item()), fontsize=11)\n",
    "    plt.title(f\"Step {step} | Loss {loss_val:.2f}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(train_iter): # number of training iterations, each iteration is one batch of training examples\n",
    "    idx = torch.randint(0, all_seqs.shape[0], (batch_size,)) # random rows of context to process. also, part of the benifit of looking at parts of instead of the whole file is kind of like cnn translation invariance where it can learn patterns regardless of where they are in the context window, like learning cat ear shape no matter it is in an image. shape (batch_size,) because it's a list of row indices to select for the batch\n",
    "    x = all_seqs[idx, :-1] # shape (batch_size, ctx_len-1) grabs random row, all elements in that row except the last, to predict all row elements in y (y is same indeices in x but shifted by + 1)\n",
    "    y = all_seqs[idx, 1:] # grabs same row as x, grabs same elements as x but shifted by + 1, so each element becomes what the transformer should predict, aka next token prediction. each y element is the correct next token.\n",
    "\n",
    "    logits = forward(x) # raw results from the transformer. makes tensors same shape, ignores dim 0, does regular matmul, then puts dim 0 in resulting tensor. shape (batch_size, ctx_len-1, vocab_size) because forward returns x @ emb.T and x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    # print(f'{logits.shape=}')\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1)) # logits becomes shape rows = every prediction in batch and every context len prediction in that batch, columns are each vocab letter, y indexes into the target index of each prediction. then each of those goes into negative natural log, then all the outputs are averaged, that is the loss.\n",
    "\n",
    "    opt.zero_grad() # makes all grads 0 so they don't add (or accumulate) to previous batch of grads\n",
    "    loss.backward() # computes gradients (partial derivatives of loss w.r.t. each parameter) via backpropagation, stores them in each tensor's .grad\n",
    "    opt.step() #updates weights with grads from loss.backward()\n",
    "\n",
    "    if i % 100 == 0: # every 100 iterations print loss\n",
    "        print(f\"{i}: loss={loss.item():.2f}\") # loss.item() gets the scalar value of the loss tensor for printing, and formats it to 2 decimal places\n",
    "\n",
    "    if i % plot_every == 0: # plot embedding map every plot_every iterations, scroll to see improvement over time\n",
    "        plot_embeddings(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "ctx = \"most important\"\n",
    "tokens = encode(ctx)\n",
    "print(tokens)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
