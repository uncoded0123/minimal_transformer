{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "batch_size = 64\n",
    "dim = 128 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights\n",
    "\n",
    "\n",
    "# 1st Block Weights\n",
    "# -----------------------------------------------\n",
    "# 1st attention\n",
    "wq0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for q, input dim is rows and it's for dot producting with nx (RMS normalized x) row vector\n",
    "wk0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for k, same as q\n",
    "wv0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for v, same as q and k\n",
    "\n",
    "# 1st MLP\n",
    "w10 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP input to hidden layer\n",
    "w20 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2nd Block Weights\n",
    "# -----------------------------------------------\n",
    "# 2nd Attention\n",
    "wq1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # takes in the output from block 0\n",
    "wk1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1\n",
    "wv1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1 and wk1\n",
    "\n",
    "# 2nd MLP\n",
    "w11 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in block 1, input to hidden layer\n",
    "w21 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in block 1, hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] â€” the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "\n",
    "\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1st Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    # Formula for attention scores: (Q @ K^T) / sqrt(d_k), where Q is the query matrix, K is the key matrix, and d_k is the dimension of the key vectors. This formula computes the attention scores by taking the dot product of the query and key matrices, and then scaling it by the square root of the dimension of the key vectors to prevent large values that can lead to exploding softmax\n",
    "    q, k, v = nx @ wq0, nx @ wk0, nx @ wv0\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # creates matrix filled with 1s in the lower triangle and diagonal, and 0s in the upper triangle.\n",
    "    scores = scores.masked_fill(~mask, float('-inf')) # ~ is bitwise not. Here it flips the mask so that the lower triangle and diagonal are False, and the upper triangle is True. Then masked_fill replaces the True values (upper triangle) with -inf, which effectively masks out those positions in the attention scores. Then it leaves everything else alone\n",
    "    sm = F.softmax(scores, dim=-1) # probabilities per row\n",
    "    attn_out = sm @ v # shape (B, T, C) because sm is (B, T, T) and v is (B, T, C). This is the output of the attention mechanism\n",
    "    x = x + attn_out\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    x = x + (nx @ w10).relu() @ w20\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2nd Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq1, nx @ wk1, nx @ wv1\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    x = x + F.softmax(scores, dim=-1) @ v\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    x = x + (nx @ w11).relu() @ w21\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    return x @ emb.T\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward)\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
