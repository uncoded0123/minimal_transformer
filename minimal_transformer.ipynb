{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "batch_size = 64\n",
    "dim = 128 # embedding vector (aka token vector) length\n",
    "n_layers = 2 # number of transformer blocks\n",
    "mlp_dim = 256 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights\n",
    "\n",
    "\n",
    "# 1st Block Weights\n",
    "# -----------------------------------------------\n",
    "# 1st attention\n",
    "wq0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for q, input dim is rows and it's for dot producting with nx (RMS normalized x) row vector\n",
    "wk0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for k, same as q\n",
    "wv0 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for v, same as q and k\n",
    "\n",
    "# 1st MLP\n",
    "w10 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP input to hidden layer\n",
    "w20 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2nd Block Weights\n",
    "# -----------------------------------------------\n",
    "# 2nd Attention\n",
    "wq1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # takes in the output from 1st block\n",
    "wk1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1\n",
    "wv1 = torch.randn(128, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # same as wq1 and wk1\n",
    "\n",
    "# 2nd MLP\n",
    "w11 = torch.randn(128, 256, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, input to hidden layer\n",
    "w21 = torch.randn(256, 128, device=device, dtype=dtype) * (0.1 / (2 ** 0.5)) # weight matrix for MLP in 2nd, hidden layer to output\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = [emb, pos, wq0, wk0, wv0, w10, w20, wq1, wk1, wv1, w11, w21]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "\n",
    "\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1st Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # Attention\n",
    "    # Formula for attention scores: (Q @ K^T) / sqrt(d_k), where Q is the query matrix, K is the key matrix, and d_k is the dimension of the key vectors. This formula computes the attention scores by taking the dot product of the query and key matrices, and then scaling it by the square root of the dimension of the key vectors to prevent large values that can lead to exploding softmax\n",
    "    q, k, v = nx @ wq0, nx @ wk0, nx @ wv0\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5) # actual attention calculations\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # creates matrix filled with 1s in the lower triangle and diagonal, and 0s in the upper triangle.\n",
    "    scores = scores.masked_fill(~mask, float('-inf')) # ~ is bitwise not. Here it flips the mask so that the lower triangle and diagonal are False, and the upper triangle is True. Then masked_fill replaces the True values (upper triangle) with -inf, which effectively masks out those positions in the attention scores. Then it leaves everything else alone\n",
    "    sm = F.softmax(scores, dim=-1) # probabilities per row\n",
    "    attn_out = sm @ v # shape (B, T, C) because sm is (B, T, T) and v is (B, T, C). This is the output of the attention mechanism\n",
    "    x = x + attn_out # residual connection, adds attn input to attn output, shape (B, T, 128)\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w10 # shape (B, T, 256)\n",
    "    out = out.relu() # shape (B, T, 256), applies ReLU activation function element-wise\n",
    "    out = out @ w20 # shape (B, T, 128), this is the output of the MLP\n",
    "    x = x + out # residual connection, adds mlp input to mlp output, shape (B, T, 128)\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2nd Block\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x) # x is input from 1st block, shape (B, T, 128)\n",
    "\n",
    "    # Attention\n",
    "    q, k, v = nx @ wq1, nx @ wk1, nx @ wv1\n",
    "    scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    sm = F.softmax(scores, dim=-1)\n",
    "    attn_out = sm @ v\n",
    "    x = x + attn_out\n",
    "\n",
    "    # Norm\n",
    "    nx = rmsnorm(x)\n",
    "\n",
    "    # MLP\n",
    "    out = nx @ w11\n",
    "    out = out.relu()\n",
    "    out = out @ w21\n",
    "    x = x + out\n",
    "    # -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    return x @ emb.T # x is prediction vector, that vector dots with emb.T to get similarity scores (dot product), which is most similar can be next token prediction. shape (B, T, vocab_size) because x is (B, T, 128) and emb.T is (128, vocab_size)\n",
    "    '''\n",
    "    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\n",
    "    Say dim=3, vocab=3 for simplicity:\n",
    "\n",
    "    emb = [[0.2, 0.5, 0.1],   # token \"a\"\n",
    "        [0.9, 0.1, 0.3],   # token \"b\"  \n",
    "        [0.1, 0.8, 0.2]]   # token \"c\"\n",
    "    Output vector after all blocks: x = [0.85, 0.15, 0.28]\n",
    "\n",
    "    x @ emb.T = dot product of x with each row:\n",
    "\n",
    "    vs \"a\": 0.85*0.2 + 0.15*0.5 + 0.28*0.1 = 0.27\n",
    "    vs \"b\": 0.85*0.9 + 0.15*0.1 + 0.28*0.3 = 0.86 ← highest\n",
    "    vs \"c\": 0.85*0.1 + 0.15*0.8 + 0.28*0.2 = 0.26\n",
    "    Result: [0.27, 0.86, 0.26] → model predicts \"b\" because x is most similar to \"b\"'s embedding.\n",
    "\n",
    "    The blocks learned to push x toward the embedding of the correct next token.\n",
    "    '''\n",
    "\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward) # torch compile makes less trips to and from vram, and optimizes the forward function in various ways, like fusing operations together, which can speed up training\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-4, fused=True)\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
