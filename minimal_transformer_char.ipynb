{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "emb_vec_len = 64 # emb vector length\n",
    "n_transformer_blocks = 4 # number of transformer blocks\n",
    "mlp_dim = 32 # hidden layer size in MLP\n",
    "ctx_len = 128 # max number of tokens model looks at in one go including itself, some may be zeroed out in causal attention\n",
    "batch_size = 32 # number of sequences per batch\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # how many unique tokens (characters for this version)\n",
    "stoi = {c:i for i,c in enumerate(chars)} # makes dict for token to int id\n",
    "itos = {i:c for c,i in stoi.items()} # makes dict for id_int to token_char\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take string, output list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take list of integers, output string\n",
    "print(f\"Vocab size: {vocab_size}, emb_vec_len: {emb_vec_len}, transformer blocks: {n_transformer_blocks}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, emb_vec_len, device=device, dtype=dtype) * 0.02 # matrix of emb vectors (picture each row is an emb vector)\n",
    "pos = torch.randn(ctx_len, emb_vec_len, device=device, dtype=dtype) * 0.02 # matrix of pos emb vectors (picture 1d... pos like 3 says you start here, that 1D emb element like 5 added tells where it ends, like 5+3=8, and positive eg tells what direction. does for each dim so nudges the vector in specifid directions.)\n",
    "\n",
    "layers = []\n",
    "for _ in range(n_transformer_blocks):\n",
    "    layer = {\n",
    "        'wq': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'wk': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'wv': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'w1': torch.randn(emb_vec_len, mlp_dim, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'w2': torch.randn(mlp_dim, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "    }\n",
    "    layers.append(layer)\n",
    "\n",
    "params = [emb, pos] + [w for layer in layers for w in layer.values()]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps) # denominator is like standar deviation except it subtracts 0 instead of mean\n",
    "\n",
    "def forward(x): # is a matrix of token ids. x is batch of 1d loodup id's, aka x is 2d, but just break it down and think in terms of 1d because the other d is just for parallelization\n",
    "    B, T = x.shape # B = batch size, T = time = sequence length (number of tokens in each sequence in the batch, = ctx_len - 1, because it predicts the next token)\n",
    "    x = emb[x] + pos[:T] # x = id's, 1 id grabs 1 emb vector from the emb matrix\n",
    "\n",
    "    for layer in layers:\n",
    "        nx = rmsnorm(x)\n",
    "        q, k, v = nx @ layer['wq'], nx @ layer['wk'], nx @ layer['wv']\n",
    "        x = x + F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        x = x + (rmsnorm(x) @ layer['w1']).relu() @ layer['w2']\n",
    "\n",
    "    return x @ emb.T\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward)\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=1e-3, fused=True) # optimizer = update weights eg.: w-= lr*gradient\n",
    "\n",
    "for i in range(1000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen fea then potarerd yo,\n",
      "Thas irter ars. buth's are an thet hentl for eth twath coold erses and, and whoure s;\n",
      "Siban wond beth af har to theen ling ceen\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "\n",
    "for _ in range(150):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
