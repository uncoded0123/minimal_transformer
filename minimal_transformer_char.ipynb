{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-06 13:39:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  2.69MB/s    in 0.4s    \n",
      "\n",
      "2026-02-06 13:39:04 (2.69 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n",
      "Using: cpu, dtype: torch.float32\n",
      "Vocab size: 65, emb_vec_len: 256, transformer blocks: 16\n",
      "Using torch.compile\n",
      "0: loss=4.25\n",
      "1: loss=4.07\n",
      "2: loss=4.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, all_seqs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), (batch_size,))\n\u001b[1;32m     99\u001b[0m x, y \u001b[38;5;241m=\u001b[39m all_seqs[idx, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], all_seqs[idx, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 101\u001b[0m logits \u001b[38;5;241m=\u001b[39m forward(x)\n\u001b[1;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    104\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmsnorm\u001b[39m(x, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m/\u001b[39m ((x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;66;03m# denominator is like standar deviation except it subtracts 0 instead of mean\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(x): \u001b[38;5;66;03m# is a matrix of token ids. x is batch of 1d loodup id's, aka x is 2d, but just break it down and think in terms of 1d because the other d is just for parallelization\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     B, T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# B = batch size, T = time = sequence length (number of tokens in each sequence in the batch, = ctx_len - 1, because it predicts the next token)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m emb[x] \u001b[38;5;241m+\u001b[39m pos[:T] \u001b[38;5;66;03m# this is where pos vector is added to emb vector... kind of like adding b in y = wx + b... x = id's, 1 id grabs 1 emb vector from the emb matrix ... \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1209\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1208\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:315\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     ), torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 315\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m    316\u001b[0m             compiled_fn, args_, disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp, steal_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     grad_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1937\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   1928\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39mfwd_rng_states)\n\u001b[1;32m   1930\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[0;32m-> 1937\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m   1938\u001b[0m     CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_fw,\n\u001b[1;32m   1939\u001b[0m     args,\n\u001b[1;32m   1940\u001b[0m     disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   1941\u001b[0m )\n\u001b[1;32m   1943\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   1944\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    488\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    489\u001b[0m         runtime_metadata,\n\u001b[1;32m    490\u001b[0m         out,\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(runtime_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:460\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable(inputs)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/.cache/pip_tmp/torchinductor_cody/df/cdfmkef46ur7zedei4drqihm464vfogq3hcaan3qyig5ex2uqfnj.py:5439\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   5437\u001b[0m buf207 \u001b[38;5;241m=\u001b[39m buf184; \u001b[38;5;28;01mdel\u001b[39;00m buf184  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[1;32m   5438\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [q_11], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m-> 5439\u001b[0m extern_kernels\u001b[38;5;241m.\u001b[39mmm(reinterpret_tensor(buf206, (\u001b[38;5;241m254\u001b[39m, \u001b[38;5;241m256\u001b[39m), (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m), primals_59, out\u001b[38;5;241m=\u001b[39mbuf207)\n\u001b[1;32m   5440\u001b[0m buf208 \u001b[38;5;241m=\u001b[39m reinterpret_tensor(buf178, (\u001b[38;5;241m254\u001b[39m, \u001b[38;5;241m256\u001b[39m), (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf178  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[1;32m   5441\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [k_11], Original ATen: [aten.mm]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss 1.34, trained on A100 40GB in about 10 minutes in 4153 iterations\n",
    "\n",
    "# --------------- train -----------------\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "lr = 5e-4\n",
    "emb_vec_len = 256 # emb vector length\n",
    "n_transformer_blocks = 16 # number of transformer blocks\n",
    "mlp_dim = 128 # hidden layer size in MLP\n",
    "ctx_len = 128 # max number of tokens model looks at in one go including itself, some may be zeroed out in causal attention\n",
    "# batch_size = 4096 # for 40GB A100\n",
    "batch_size = 2 # number of sequences per batch\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) # how many unique tokens (characters for this version)\n",
    "stoi = {c:i for i,c in enumerate(chars)} # makes dict for token to int id\n",
    "itos = {i:c for c,i in stoi.items()} # makes dict for id_int to token_char\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take string, output list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take list of integers, output string\n",
    "print(f\"Vocab size: {vocab_size}, emb_vec_len: {emb_vec_len}, transformer blocks: {n_transformer_blocks}\")\n",
    "\n",
    "# weights\n",
    "emb = torch.randn(vocab_size, emb_vec_len, device=device, dtype=dtype) * 0.02 # matrix of emb vectors (picture each row is an emb vector)\n",
    "pos = torch.randn(ctx_len, emb_vec_len, device=device, dtype=dtype) * 0.02 # matrix of pos emb vectors (picture 1d... pos like 3 says you start here, that 1D emb element like 5 added tells where it ends, like 5+3=8, and positive eg tells what direction. does for each dim so nudges the vector in specifid directions.)\n",
    "\n",
    "layers = []\n",
    "for _ in range(n_transformer_blocks):\n",
    "    layer = {\n",
    "        'wq': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'wk': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'wv': torch.randn(emb_vec_len, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'w1': torch.randn(emb_vec_len, mlp_dim, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "        'w2': torch.randn(mlp_dim, emb_vec_len, device=device, dtype=dtype) * (0.1 / (n_transformer_blocks ** 0.5)),\n",
    "    }\n",
    "    layers.append(layer)\n",
    "\n",
    "params = [emb, pos] + [w for layer in layers for w in layer.values()]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps) # denominator is like standar deviation except it subtracts 0 instead of mean\n",
    "\n",
    "def forward(x): # is a matrix of token ids. x is batch of 1d loodup id's, aka x is 2d, but just break it down and think in terms of 1d because the other d is just for parallelization\n",
    "    B, T = x.shape # B = batch size, T = time = sequence length (number of tokens in each sequence in the batch, = ctx_len - 1, because it predicts the next token)\n",
    "    x = emb[x] + pos[:T] # this is where pos vector is added to emb vector... kind of like adding b in y = wx + b... x = id's, 1 id grabs 1 emb vector from the emb matrix ... \n",
    "\n",
    "    for layer in layers:\n",
    "        nx = rmsnorm(x) # scales x\n",
    "        q, k, v = nx @ layer['wq'], nx @ layer['wk'], nx @ layer['wv']\n",
    "        scale = q.shape[-1] ** 0.5 # scales scores\n",
    "        scores = q @ k.transpose(-2, -1) / scale\n",
    "        \n",
    "        # triangular causal mask\n",
    "        rows = torch.arange(T, device=device).unsqueeze(1) # (T, 1)\n",
    "        cols = torch.arange(T, device=device).unsqueeze(0) # (1, T)\n",
    "        mask = cols <= rows # True where col <= row (lower triangle)\n",
    "        scores = torch.where(mask, scores, torch.tensor(float('-inf'), device=device)) # keep score if mask=True, else -inf so softmax gives 0\n",
    "        \n",
    "        # print(f'rows: {rows.shape}, cols: {cols.shape}, mask: {mask.shape}, scores: {scores.shape}, T: {T}')\n",
    "\n",
    "        x = x + F.softmax(scores, dim=-1) @ v\n",
    "        x = x + (rmsnorm(x) @ layer['w1']).relu() @ layer['w2']\n",
    "\n",
    "    return x @ emb.T\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward)\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device)\n",
    "all_seqs = tokens.unfold(0, ctx_len, 1)\n",
    "\n",
    "# train\n",
    "opt = torch.optim.Adam(params, lr=lr, fused=True) # optimizer = update weights eg.: w-= lr*gradient\n",
    "\n",
    "for i in range(10000):\n",
    "    idx = torch.randint(0, all_seqs.size(0), (batch_size,))\n",
    "    x, y = all_seqs[idx, :-1], all_seqs[idx, 1:]\n",
    "\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")\n",
    "\n",
    "'''\n",
    "loss = 1.34\n",
    "# config\n",
    "lr = 5e-4\n",
    "emb_vec_len = 256 # emb vector length\n",
    "n_transformer_blocks = 16 # number of transformer blocks\n",
    "mlp_dim = 128 # hidden layer size in MLP\n",
    "ctx_len = 128 # max number of tokens model looks at in one go including itself, some may be zeroed out in causal attention\n",
    "batch_size = 4096 # number of sequences per batch\n",
    "\n",
    "Eg. of output from prompt 'First Citizens':\n",
    "\n",
    "First Citizens of found him appearl with me intood\n",
    "not the lifter removetle like a storm'd;\n",
    "And fellong where tha\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n",
      "eiirseAi'Al\n",
      "kiy\n",
      ";iBI\n",
      "Kvee\n",
      "dBe.riT$iiAw\n",
      "yApmAmr,cRemxekTd\n",
      "g3ecRRRT&oeiTbekwd:iM\n",
      "o-m\n",
      "\n",
      "ieewAo\n",
      "\n",
      "gzeieet\n"
     ]
    }
   ],
   "source": [
    "# -------------- inference generate -----------------\n",
    "ctx = \"First Citizen\"\n",
    "tokens = encode(ctx)\n",
    "\n",
    "for _ in range(100):\n",
    "    seq = tokens[-(ctx_len-1):]\n",
    "    pad_len = (ctx_len - 1) - len(seq)\n",
    "    x = torch.tensor([([0] * pad_len) + seq], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
