{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu, dtype: torch.float32\n",
      "Vocab size: 65, dim: 256, layers: 8\n",
      "Using torch.compile\n"
     ]
    }
   ],
   "source": [
    "import torch # torch is a folder, __init__.py in that torch folder imports other .py files in that torch folder, i can then import any of those imports with torch dot whatever\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# uncomment for file download:\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda' and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif device == 'cuda':\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using: {device}, dtype: {dtype}\")\n",
    "\n",
    "# config\n",
    "lr = 3e-4\n",
    "train_iter = 1000000\n",
    "batch_size = 8\n",
    "dim = 256 # embedding vector (aka token vector) length\n",
    "n_layers = 8 # number of transformer blocks\n",
    "mlp_dim = 4096 # hidden layer 1 nodes in MLP\n",
    "ctx_len = 32 # how many tokens it looks at, at once\n",
    "plot_every = 5000 # plot embedding map every N iterations\n",
    "\n",
    "# load data\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# char tokenizer\n",
    "chars = sorted(set(text)) # set finds unique chars, sorted puts them in order\n",
    "vocab_size = len(chars) # number of unique tokens in the vocabulary\n",
    "stoi = {c:i for i,c in enumerate(chars)} # stoi is a dictionary. this loop (or dict comprehension) built it. from then on, stoi now is dict like {'a': 0, 'b': 1, ...}\n",
    "itos = {i:c for c,i in stoi.items()} # itos is the reverse of stoi. itos is like {0: 'a', 1: 'b', ...}\n",
    "encode = lambda s: [stoi[c] for c in s] # makes list, gets int (id) from stoi dictionary by single char as key, that char came from string. like s = abc, c = a first, then c = b, etc.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # reverse of encode, and makes list into string by using ''.join\n",
    "print(f\"Vocab size: {vocab_size}, dim: {dim}, layers: {n_layers}\")\n",
    "\n",
    "emb = torch.randn(vocab_size, dim, device=device, dtype=dtype) * 0.02 # creates emb matrix where each row represents 1 tokens embedding vector\n",
    "pos = torch.randn(ctx_len, dim, device=device, dtype=dtype) * 0.02 # same as emb but for positional embeddings. each row represents 1 position in the context window, and the values in that row are the positional embedding elements for that position. pos[0] is the positional embedding for the first token in the context window, pos[1] is for the second token, etc.\n",
    "\n",
    "\n",
    "# Transformer block weights — lists of weight matrices, one per layer\n",
    "# each layer gets its own wq, wk, wv (attention) and w1, w2 (MLP)\n",
    "init_scale = 0.1 / (2 ** 0.5)\n",
    "wq = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wq[layer] = query weight matrix\n",
    "wk = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wk[layer] = key weight matrix\n",
    "wv = [torch.randn(dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # wv[layer] = value weight matrix\n",
    "w1 = [torch.randn(dim, mlp_dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # w1[layer] = MLP input→hidden\n",
    "w2 = [torch.randn(mlp_dim, dim, device=device, dtype=dtype) * init_scale for _ in range(n_layers)] # w2[layer] = MLP hidden→output\n",
    "\n",
    "\n",
    "params = [emb, pos] + wq + wk + wv + w1 + w2\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def rmsnorm(x, eps=1e-5):\n",
    "    return x / ((x ** 2).mean(dim=-1, keepdim=True).sqrt() + eps)\n",
    "\n",
    "def forward(x):\n",
    "    B, T = x.shape # x.shape(batch_size, ctx_len-1) B = batch size, T = sequence length (context length - 1). Because x = all_seqs[idx, :-1] — the last token is sliced off to become the target y\n",
    "    x = emb[x] + pos[:T] # C = dim, emb[x].shape(B,T,C), pos[:T].shape(T,C), + broadcasts pos[:T] along dimension 0 for element wise add, pos[:T] indexing is up to but not including T value (slicing rules)\n",
    "    mask = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # causal mask, reused across all layers\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        # Norm\n",
    "        nx = rmsnorm(x)\n",
    "\n",
    "        # Attention\n",
    "        q, k, v = nx @ wq[layer], nx @ wk[layer], nx @ wv[layer]\n",
    "        scores = q @ k.transpose(-2, -1) / (q.size(-1) ** 0.5)\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attn_out = F.softmax(scores, dim=-1) @ v\n",
    "        x = x + attn_out # residual connection\n",
    "\n",
    "        # Norm\n",
    "        nx = rmsnorm(x)\n",
    "\n",
    "        # MLP\n",
    "        out = (nx @ w1[layer]).relu() @ w2[layer]\n",
    "        x = x + out # residual connection\n",
    "        \n",
    "    x = rmsnorm(x)\n",
    "    x = x @ emb.T # similarity scores against all token embeddings → logits\n",
    "    return x\n",
    "    \n",
    "    '''\n",
    "    one x row mul and add with each emb row, to get similarity score for each token. The highest score is the predicted next token. The blocks learn to push x toward the embedding of the correct next token, so that the dot product (similarity) is highest for the correct next token.\n",
    "    Say dim=3, vocab=3 for simplicity:\n",
    "\n",
    "    emb = [[0.2, 0.5, 0.1],   # token \"a\"\n",
    "        [0.9, 0.1, 0.3],   # token \"b\"  \n",
    "        [0.1, 0.8, 0.2]]   # token \"c\"\n",
    "    Output vector after all blocks: x = [0.85, 0.15, 0.28]\n",
    "\n",
    "    x @ emb.T = dot product of x with each row:\n",
    "\n",
    "    vs \"a\": 0.85*0.2 + 0.15*0.5 + 0.28*0.1 = 0.27\n",
    "    vs \"b\": 0.85*0.9 + 0.15*0.1 + 0.28*0.3 = 0.86 ← highest\n",
    "    vs \"c\": 0.85*0.1 + 0.15*0.8 + 0.28*0.2 = 0.26\n",
    "    Result: [0.27, 0.86, 0.26] → model predicts \"b\" because x is most similar to \"b\"'s embedding.\n",
    "\n",
    "    The blocks learned to push x toward the embedding of the correct next token.\n",
    "    '''\n",
    "\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    forward = torch.compile(forward) # compiles forward, removes python interpretor, looks at all the ops, makes less trips between vram and gpu cores with kernel fusion where instead of doing a calculation for 1 var by carrying data from vram to gpu cores doing an intermediate calculation then bringing  result back to vram then taking that answer back to cores for next operation then returning to vram over and over it fuses (combines) all those operations into 1 transfer then does the calculations then returns result to vram\n",
    "    print(\"Using torch.compile\")\n",
    "\n",
    "tokens = torch.tensor(encode(text), device=device) # encoding text to list of numbers (token ids), converting that list to a tensor\n",
    "'''\n",
    "all_seqs = [] # stores tensors in this list. shape eg. (1000000, 32) aka (all tokens in file, context len) stores like lists of all context size sequences in the fiile. So if ctx_len is 32, it stores all sequences of 32 tokens (sliding 1 token at a time) in the file, which are the training examples. Each sequence is a sequence of token ids (integers).\n",
    "for i in range(len(tokens) - ctx_len + 1): # same pattern as convolutional kernal... len of total elements minus window (or convolutional kernel) size + 1.\n",
    "    all_seqs.append(tokens[i : i + ctx_len]) # ctx len is how wide is that window or kernel. i changes by step size. makes list of tokens 0 to 31, appends all those to all_seq list, next iter looks at tokens 1 to 32, appends that list as next row of tokens, etc.\n",
    "all_seqs = torch.stack(all_seqs) # converts list of seperate tensors to one matrix tensor\n",
    "'''\n",
    "all_seqs = tokens.unfold(dimension=0, size=ctx_len, step=1) # same as torch.stack, just faster. shape (kind of like cnn number of chars in whole file like 1115390 minus window size like 32 and + 1 for the starting window position, ctx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "opt = torch.optim.Adam(params, lr=lr, fused=True)\n",
    "\n",
    "def plot_embeddings(step, loss_val):\n",
    "    E = emb.detach().float().cpu()\n",
    "    E = E - E.mean(dim=0)\n",
    "    U, S, V = torch.svd(E)\n",
    "    coords = E @ V[:, :2]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(coords[:, 0].numpy(), coords[:, 1].numpy(), s=20)\n",
    "    for i, c in itos.items():\n",
    "        label = repr(c) if c in (' ', '\\n', '\\t') else c\n",
    "        plt.annotate(label, (coords[i, 0].item(), coords[i, 1].item()), fontsize=11)\n",
    "    plt.title(f\"Step {step} | Loss {loss_val:.2f}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(train_iter):\n",
    "    idx = torch.randint(0, all_seqs.shape[0], (batch_size,))\n",
    "    x = all_seqs[idx, :-1]\n",
    "    y = all_seqs[idx, 1:]\n",
    "    logits = forward(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}: loss={loss.item():.2f}\")\n",
    "    if i % plot_every == 0:\n",
    "        plot_embeddings(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "# torch.save({\n",
    "#     'emb': emb.data, 'pos': pos.data,\n",
    "#     'wq': [w.data for w in wq], 'wk': [w.data for w in wk],\n",
    "#     'wv': [w.data for w in wv], 'w1': [w.data for w in w1],\n",
    "#     'w2': [w.data for w in w2],\n",
    "# }, 'weights.pt')\n",
    "\n",
    "# if in colab, download to pc:\n",
    "# from google.colab import files\n",
    "# files.download('weights.pt')\n",
    "\n",
    "# print(\"Saved weights.pt\")\n",
    "\n",
    "# to load later instead of training:\n",
    "ckpt = torch.load('weights.pt', map_location=device)\n",
    "emb.data = ckpt['emb'].to(dtype=dtype)\n",
    "pos.data = ckpt['pos'].to(dtype=dtype)\n",
    "for i in range(n_layers):\n",
    "    wq[i].data = ckpt['wq'][i].to(dtype=dtype)\n",
    "    wk[i].data = ckpt['wk'][i].to(dtype=dtype)\n",
    "    wv[i].data = ckpt['wv'][i].to(dtype=dtype)\n",
    "    w1[i].data = ckpt['w1'][i].to(dtype=dtype)\n",
    "    w2[i].data = ckpt['w2'][i].to(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 53, 57, 58, 1, 47, 51, 54, 53, 56, 58, 39, 52, 58]\n",
      "most important,\n",
      "Which runs of war suffice.\n",
      "\n",
      "JULIET:\n",
      "I'll swear to the Tower.\n",
      "\n",
      "PRINCE:\n",
      "Be rale, my lord, as ; many a thousand these words in this point on Kent,\n",
      "Shall I dash out my deserts in this rude assault;\n",
      "And when the trier of my death,\n",
      "To show the state some noise me from their lives,\n",
      "That I mine own hands I spied\n",
      "With betters than England? What realm\n",
      "Ymbron that dogs must be a prisoner.\n",
      "A silly king and a severable cannot round\n",
      "Juliet matter to speak from himself.\n",
      "Go, sirrah, to addget it, thou strikes\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "ctx = \"most important\"\n",
    "tokens = encode(ctx)\n",
    "print(tokens)\n",
    "\n",
    "for _ in range(500):\n",
    "    x = torch.tensor([tokens[-(ctx_len-1):]], device=device)\n",
    "    logits = forward(x)\n",
    "    probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    tokens.append(next_token)\n",
    "\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
